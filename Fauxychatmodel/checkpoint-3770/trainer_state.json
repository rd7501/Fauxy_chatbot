{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 3770,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.05308560053085601,
      "grad_norm": 0.9924201965332031,
      "learning_rate": 0.0001999874661302089,
      "loss": 2.6622,
      "step": 20
    },
    {
      "epoch": 0.10617120106171202,
      "grad_norm": 0.6676366925239563,
      "learning_rate": 0.00019994719463633997,
      "loss": 2.43,
      "step": 40
    },
    {
      "epoch": 0.15925680159256803,
      "grad_norm": 0.725319504737854,
      "learning_rate": 0.00019987916198354908,
      "loss": 2.3515,
      "step": 60
    },
    {
      "epoch": 0.21234240212342403,
      "grad_norm": 0.7597302794456482,
      "learning_rate": 0.00019978338706846756,
      "loss": 2.2686,
      "step": 80
    },
    {
      "epoch": 0.26542800265428,
      "grad_norm": 0.6315045952796936,
      "learning_rate": 0.0001996598964933692,
      "loss": 2.2577,
      "step": 100
    },
    {
      "epoch": 0.31851360318513605,
      "grad_norm": 0.6408698558807373,
      "learning_rate": 0.00019950872455878137,
      "loss": 2.3048,
      "step": 120
    },
    {
      "epoch": 0.37159920371599203,
      "grad_norm": 0.7528989911079407,
      "learning_rate": 0.00019932991325395755,
      "loss": 2.2547,
      "step": 140
    },
    {
      "epoch": 0.42468480424684807,
      "grad_norm": 0.6384961009025574,
      "learning_rate": 0.0001991235122452148,
      "loss": 2.2418,
      "step": 160
    },
    {
      "epoch": 0.47777040477770405,
      "grad_norm": 0.9508875012397766,
      "learning_rate": 0.0001988895788621383,
      "loss": 2.2044,
      "step": 180
    },
    {
      "epoch": 0.53085600530856,
      "grad_norm": 0.7279564738273621,
      "learning_rate": 0.00019862817808165748,
      "loss": 2.1918,
      "step": 200
    },
    {
      "epoch": 0.583941605839416,
      "grad_norm": 0.725348711013794,
      "learning_rate": 0.00019833938250999852,
      "loss": 2.2388,
      "step": 220
    },
    {
      "epoch": 0.6370272063702721,
      "grad_norm": 0.7739380598068237,
      "learning_rate": 0.000198023272362517,
      "loss": 2.1498,
      "step": 240
    },
    {
      "epoch": 0.6901128069011281,
      "grad_norm": 0.7155356407165527,
      "learning_rate": 0.0001976799354414176,
      "loss": 2.2105,
      "step": 260
    },
    {
      "epoch": 0.7431984074319841,
      "grad_norm": 0.6632840633392334,
      "learning_rate": 0.00019730946711136633,
      "loss": 2.2113,
      "step": 280
    },
    {
      "epoch": 0.79628400796284,
      "grad_norm": 0.7661823630332947,
      "learning_rate": 0.00019691197027300205,
      "loss": 2.2027,
      "step": 300
    },
    {
      "epoch": 0.8493696084936961,
      "grad_norm": 0.7563638091087341,
      "learning_rate": 0.00019648755533435518,
      "loss": 2.0682,
      "step": 320
    },
    {
      "epoch": 0.9024552090245521,
      "grad_norm": 0.7886581420898438,
      "learning_rate": 0.0001960363401801807,
      "loss": 2.0662,
      "step": 340
    },
    {
      "epoch": 0.9555408095554081,
      "grad_norm": 0.7104741334915161,
      "learning_rate": 0.00019555845013921505,
      "loss": 2.161,
      "step": 360
    },
    {
      "epoch": 1.0079628400796283,
      "grad_norm": 0.6727052330970764,
      "learning_rate": 0.00019505401794936468,
      "loss": 2.0844,
      "step": 380
    },
    {
      "epoch": 1.0610484406104843,
      "grad_norm": 1.1497653722763062,
      "learning_rate": 0.00019452318372083712,
      "loss": 1.8952,
      "step": 400
    },
    {
      "epoch": 1.1141340411413405,
      "grad_norm": 1.015913486480713,
      "learning_rate": 0.00019396609489722436,
      "loss": 1.8588,
      "step": 420
    },
    {
      "epoch": 1.1672196416721965,
      "grad_norm": 1.3876111507415771,
      "learning_rate": 0.00019338290621454893,
      "loss": 1.8445,
      "step": 440
    },
    {
      "epoch": 1.2203052422030525,
      "grad_norm": 0.9831981062889099,
      "learning_rate": 0.00019277377965828488,
      "loss": 1.7988,
      "step": 460
    },
    {
      "epoch": 1.2733908427339085,
      "grad_norm": 1.0083321332931519,
      "learning_rate": 0.00019213888441836486,
      "loss": 1.8565,
      "step": 480
    },
    {
      "epoch": 1.3264764432647644,
      "grad_norm": 1.1030027866363525,
      "learning_rate": 0.00019147839684218636,
      "loss": 1.853,
      "step": 500
    },
    {
      "epoch": 1.3795620437956204,
      "grad_norm": 0.8798091411590576,
      "learning_rate": 0.00019079250038562954,
      "loss": 1.888,
      "step": 520
    },
    {
      "epoch": 1.4326476443264764,
      "grad_norm": 1.0952508449554443,
      "learning_rate": 0.00019008138556210108,
      "loss": 1.7815,
      "step": 540
    },
    {
      "epoch": 1.4857332448573324,
      "grad_norm": 1.1096218824386597,
      "learning_rate": 0.00018934524988961738,
      "loss": 1.8231,
      "step": 560
    },
    {
      "epoch": 1.5388188453881884,
      "grad_norm": 1.0910847187042236,
      "learning_rate": 0.00018858429783594224,
      "loss": 1.7446,
      "step": 580
    },
    {
      "epoch": 1.5919044459190443,
      "grad_norm": 1.1076881885528564,
      "learning_rate": 0.00018779874076179447,
      "loss": 1.8467,
      "step": 600
    },
    {
      "epoch": 1.6449900464499003,
      "grad_norm": 1.3672171831130981,
      "learning_rate": 0.00018698879686214045,
      "loss": 1.8267,
      "step": 620
    },
    {
      "epoch": 1.6980756469807563,
      "grad_norm": 1.0829273462295532,
      "learning_rate": 0.00018615469110558883,
      "loss": 1.7803,
      "step": 640
    },
    {
      "epoch": 1.7511612475116125,
      "grad_norm": 1.3414020538330078,
      "learning_rate": 0.00018529665517190362,
      "loss": 1.816,
      "step": 660
    },
    {
      "epoch": 1.8042468480424685,
      "grad_norm": 1.017830729484558,
      "learning_rate": 0.0001844149273876532,
      "loss": 1.8225,
      "step": 680
    },
    {
      "epoch": 1.8573324485733245,
      "grad_norm": 1.2576392889022827,
      "learning_rate": 0.0001835097526600131,
      "loss": 1.7592,
      "step": 700
    },
    {
      "epoch": 1.9104180491041804,
      "grad_norm": 1.075850009918213,
      "learning_rate": 0.00018258138240874114,
      "loss": 1.7868,
      "step": 720
    },
    {
      "epoch": 1.9635036496350367,
      "grad_norm": 1.4095550775527954,
      "learning_rate": 0.0001816300744963434,
      "loss": 1.749,
      "step": 740
    },
    {
      "epoch": 2.0159256801592567,
      "grad_norm": 1.203525424003601,
      "learning_rate": 0.00018065609315645075,
      "loss": 1.5038,
      "step": 760
    },
    {
      "epoch": 2.0690112806901126,
      "grad_norm": 1.920928955078125,
      "learning_rate": 0.00017965970892042598,
      "loss": 1.2651,
      "step": 780
    },
    {
      "epoch": 2.1220968812209686,
      "grad_norm": 1.5301235914230347,
      "learning_rate": 0.00017864119854222116,
      "loss": 1.2998,
      "step": 800
    },
    {
      "epoch": 2.1751824817518246,
      "grad_norm": 1.2820075750350952,
      "learning_rate": 0.0001776008449215073,
      "loss": 1.3098,
      "step": 820
    },
    {
      "epoch": 2.228268082282681,
      "grad_norm": 1.510846734046936,
      "learning_rate": 0.00017653893702509633,
      "loss": 1.2312,
      "step": 840
    },
    {
      "epoch": 2.281353682813537,
      "grad_norm": 1.1992517709732056,
      "learning_rate": 0.00017545576980667837,
      "loss": 1.302,
      "step": 860
    },
    {
      "epoch": 2.334439283344393,
      "grad_norm": 1.9134573936462402,
      "learning_rate": 0.0001743516441248958,
      "loss": 1.2742,
      "step": 880
    },
    {
      "epoch": 2.387524883875249,
      "grad_norm": 1.590842843055725,
      "learning_rate": 0.00017322686665977737,
      "loss": 1.3265,
      "step": 900
    },
    {
      "epoch": 2.440610484406105,
      "grad_norm": 1.5833635330200195,
      "learning_rate": 0.00017208174982755517,
      "loss": 1.3001,
      "step": 920
    },
    {
      "epoch": 2.493696084936961,
      "grad_norm": 1.470043659210205,
      "learning_rate": 0.0001709166116938886,
      "loss": 1.2362,
      "step": 940
    },
    {
      "epoch": 2.546781685467817,
      "grad_norm": 1.9295190572738647,
      "learning_rate": 0.00016973177588551884,
      "loss": 1.2684,
      "step": 960
    },
    {
      "epoch": 2.599867285998673,
      "grad_norm": 1.6481364965438843,
      "learning_rate": 0.00016852757150037902,
      "loss": 1.2327,
      "step": 980
    },
    {
      "epoch": 2.652952886529529,
      "grad_norm": 1.541979193687439,
      "learning_rate": 0.00016730433301618444,
      "loss": 1.3284,
      "step": 1000
    },
    {
      "epoch": 2.706038487060385,
      "grad_norm": 1.8396813869476318,
      "learning_rate": 0.00016606240019752857,
      "loss": 1.2429,
      "step": 1020
    },
    {
      "epoch": 2.759124087591241,
      "grad_norm": 1.8470789194107056,
      "learning_rate": 0.00016480211800151077,
      "loss": 1.2456,
      "step": 1040
    },
    {
      "epoch": 2.812209688122097,
      "grad_norm": 2.023202896118164,
      "learning_rate": 0.00016352383648192152,
      "loss": 1.2684,
      "step": 1060
    },
    {
      "epoch": 2.865295288652953,
      "grad_norm": 1.9154572486877441,
      "learning_rate": 0.00016222791069201207,
      "loss": 1.2343,
      "step": 1080
    },
    {
      "epoch": 2.918380889183809,
      "grad_norm": 1.8082302808761597,
      "learning_rate": 0.0001609147005858756,
      "loss": 1.2735,
      "step": 1100
    },
    {
      "epoch": 2.9714664897146648,
      "grad_norm": 1.7430329322814941,
      "learning_rate": 0.00015958457091846677,
      "loss": 1.3055,
      "step": 1120
    },
    {
      "epoch": 3.023888520238885,
      "grad_norm": 3.737795829772949,
      "learning_rate": 0.00015823789114428825,
      "loss": 1.1039,
      "step": 1140
    },
    {
      "epoch": 3.076974120769741,
      "grad_norm": 2.0935680866241455,
      "learning_rate": 0.00015687503531477154,
      "loss": 0.8163,
      "step": 1160
    },
    {
      "epoch": 3.130059721300597,
      "grad_norm": 1.9684972763061523,
      "learning_rate": 0.0001554963819743811,
      "loss": 0.7215,
      "step": 1180
    },
    {
      "epoch": 3.183145321831453,
      "grad_norm": 1.9988535642623901,
      "learning_rate": 0.00015410231405547063,
      "loss": 0.7774,
      "step": 1200
    },
    {
      "epoch": 3.236230922362309,
      "grad_norm": 2.1822149753570557,
      "learning_rate": 0.00015269321877192015,
      "loss": 0.724,
      "step": 1220
    },
    {
      "epoch": 3.289316522893165,
      "grad_norm": 2.4284965991973877,
      "learning_rate": 0.0001512694875115846,
      "loss": 0.7445,
      "step": 1240
    },
    {
      "epoch": 3.342402123424021,
      "grad_norm": 1.9822577238082886,
      "learning_rate": 0.00014983151572758218,
      "loss": 0.7798,
      "step": 1260
    },
    {
      "epoch": 3.395487723954877,
      "grad_norm": 2.059730052947998,
      "learning_rate": 0.0001483797028284542,
      "loss": 0.7506,
      "step": 1280
    },
    {
      "epoch": 3.448573324485733,
      "grad_norm": 2.5388903617858887,
      "learning_rate": 0.00014691445206722612,
      "loss": 0.78,
      "step": 1300
    },
    {
      "epoch": 3.5016589250165895,
      "grad_norm": 2.3345532417297363,
      "learning_rate": 0.00014543617042940055,
      "loss": 0.8496,
      "step": 1320
    },
    {
      "epoch": 3.554744525547445,
      "grad_norm": 1.7212053537368774,
      "learning_rate": 0.00014394526851991364,
      "loss": 0.7924,
      "step": 1340
    },
    {
      "epoch": 3.6078301260783014,
      "grad_norm": 1.548073172569275,
      "learning_rate": 0.0001424421604490864,
      "loss": 0.8099,
      "step": 1360
    },
    {
      "epoch": 3.6609157266091574,
      "grad_norm": 1.7546555995941162,
      "learning_rate": 0.00014092726371760182,
      "loss": 0.8484,
      "step": 1380
    },
    {
      "epoch": 3.7140013271400134,
      "grad_norm": 2.2978453636169434,
      "learning_rate": 0.00013940099910054085,
      "loss": 0.7908,
      "step": 1400
    },
    {
      "epoch": 3.7670869276708694,
      "grad_norm": 1.5005189180374146,
      "learning_rate": 0.00013786379053050865,
      "loss": 0.7427,
      "step": 1420
    },
    {
      "epoch": 3.8201725282017254,
      "grad_norm": 2.0675365924835205,
      "learning_rate": 0.0001363160649798835,
      "loss": 0.8111,
      "step": 1440
    },
    {
      "epoch": 3.8732581287325814,
      "grad_norm": 1.847796082496643,
      "learning_rate": 0.0001347582523422221,
      "loss": 0.8255,
      "step": 1460
    },
    {
      "epoch": 3.9263437292634373,
      "grad_norm": 2.2251334190368652,
      "learning_rate": 0.00013319078531285285,
      "loss": 0.7967,
      "step": 1480
    },
    {
      "epoch": 3.9794293297942933,
      "grad_norm": 2.1390299797058105,
      "learning_rate": 0.00013161409926869117,
      "loss": 0.8262,
      "step": 1500
    },
    {
      "epoch": 4.031851360318513,
      "grad_norm": 2.6203463077545166,
      "learning_rate": 0.00013002863214731,
      "loss": 0.5475,
      "step": 1520
    },
    {
      "epoch": 4.08493696084937,
      "grad_norm": 1.993364691734314,
      "learning_rate": 0.00012843482432529904,
      "loss": 0.4501,
      "step": 1540
    },
    {
      "epoch": 4.138022561380225,
      "grad_norm": 1.8196909427642822,
      "learning_rate": 0.00012683311849594645,
      "loss": 0.4192,
      "step": 1560
    },
    {
      "epoch": 4.191108161911082,
      "grad_norm": 1.7823786735534668,
      "learning_rate": 0.00012522395954627722,
      "loss": 0.4526,
      "step": 1580
    },
    {
      "epoch": 4.244193762441937,
      "grad_norm": 2.680824041366577,
      "learning_rate": 0.000123607794433482,
      "loss": 0.4354,
      "step": 1600
    },
    {
      "epoch": 4.297279362972794,
      "grad_norm": 1.6632438898086548,
      "learning_rate": 0.00012198507206077121,
      "loss": 0.4089,
      "step": 1620
    },
    {
      "epoch": 4.350364963503649,
      "grad_norm": 2.301825761795044,
      "learning_rate": 0.00012035624315268832,
      "loss": 0.4475,
      "step": 1640
    },
    {
      "epoch": 4.403450564034506,
      "grad_norm": 1.998435378074646,
      "learning_rate": 0.00011872176012991743,
      "loss": 0.4753,
      "step": 1660
    },
    {
      "epoch": 4.456536164565362,
      "grad_norm": 2.432154655456543,
      "learning_rate": 0.00011708207698361977,
      "loss": 0.4419,
      "step": 1680
    },
    {
      "epoch": 4.509621765096218,
      "grad_norm": 2.0923714637756348,
      "learning_rate": 0.00011543764914933375,
      "loss": 0.4372,
      "step": 1700
    },
    {
      "epoch": 4.562707365627074,
      "grad_norm": 2.9569807052612305,
      "learning_rate": 0.00011378893338047414,
      "loss": 0.4143,
      "step": 1720
    },
    {
      "epoch": 4.6157929661579296,
      "grad_norm": 1.428727149963379,
      "learning_rate": 0.0001121363876214649,
      "loss": 0.4539,
      "step": 1740
    },
    {
      "epoch": 4.668878566688786,
      "grad_norm": 1.8789570331573486,
      "learning_rate": 0.00011048047088054143,
      "loss": 0.4534,
      "step": 1760
    },
    {
      "epoch": 4.7219641672196415,
      "grad_norm": 2.5681276321411133,
      "learning_rate": 0.00010882164310225718,
      "loss": 0.4681,
      "step": 1780
    },
    {
      "epoch": 4.775049767750498,
      "grad_norm": 1.9419538974761963,
      "learning_rate": 0.00010716036503973028,
      "loss": 0.4602,
      "step": 1800
    },
    {
      "epoch": 4.8281353682813535,
      "grad_norm": 1.9593873023986816,
      "learning_rate": 0.00010549709812666555,
      "loss": 0.4386,
      "step": 1820
    },
    {
      "epoch": 4.88122096881221,
      "grad_norm": 2.420954704284668,
      "learning_rate": 0.0001038323043491875,
      "loss": 0.4231,
      "step": 1840
    },
    {
      "epoch": 4.934306569343065,
      "grad_norm": 2.4202327728271484,
      "learning_rate": 0.00010216644611751975,
      "loss": 0.456,
      "step": 1860
    },
    {
      "epoch": 4.987392169873922,
      "grad_norm": 2.822319507598877,
      "learning_rate": 0.00010049998613754696,
      "loss": 0.4944,
      "step": 1880
    },
    {
      "epoch": 5.039814200398142,
      "grad_norm": 1.9017642736434937,
      "learning_rate": 9.883338728229443e-05,
      "loss": 0.2548,
      "step": 1900
    },
    {
      "epoch": 5.092899800928998,
      "grad_norm": 1.0364558696746826,
      "learning_rate": 9.716711246336092e-05,
      "loss": 0.2317,
      "step": 1920
    },
    {
      "epoch": 5.145985401459854,
      "grad_norm": 2.041882038116455,
      "learning_rate": 9.550162450234179e-05,
      "loss": 0.2212,
      "step": 1940
    },
    {
      "epoch": 5.19907100199071,
      "grad_norm": 1.7349933385849,
      "learning_rate": 9.383738600227585e-05,
      "loss": 0.2239,
      "step": 1960
    },
    {
      "epoch": 5.252156602521566,
      "grad_norm": 2.0730721950531006,
      "learning_rate": 9.217485921915392e-05,
      "loss": 0.2372,
      "step": 1980
    },
    {
      "epoch": 5.305242203052422,
      "grad_norm": 1.939213514328003,
      "learning_rate": 9.05145059335233e-05,
      "loss": 0.2336,
      "step": 2000
    },
    {
      "epoch": 5.358327803583278,
      "grad_norm": 1.8234220743179321,
      "learning_rate": 8.885678732222435e-05,
      "loss": 0.2529,
      "step": 2020
    },
    {
      "epoch": 5.411413404114134,
      "grad_norm": 1.7358955144882202,
      "learning_rate": 8.720216383029491e-05,
      "loss": 0.2404,
      "step": 2040
    },
    {
      "epoch": 5.46449900464499,
      "grad_norm": 0.8857895135879517,
      "learning_rate": 8.55510950430779e-05,
      "loss": 0.2255,
      "step": 2060
    },
    {
      "epoch": 5.517584605175846,
      "grad_norm": 1.5755672454833984,
      "learning_rate": 8.39040395585675e-05,
      "loss": 0.2395,
      "step": 2080
    },
    {
      "epoch": 5.570670205706702,
      "grad_norm": 1.8694034814834595,
      "learning_rate": 8.226145486003008e-05,
      "loss": 0.2509,
      "step": 2100
    },
    {
      "epoch": 5.623755806237558,
      "grad_norm": 1.5849007368087769,
      "learning_rate": 8.062379718893417e-05,
      "loss": 0.2412,
      "step": 2120
    },
    {
      "epoch": 5.676841406768414,
      "grad_norm": 1.5110610723495483,
      "learning_rate": 7.899152141822574e-05,
      "loss": 0.2455,
      "step": 2140
    },
    {
      "epoch": 5.7299270072992705,
      "grad_norm": 1.5647375583648682,
      "learning_rate": 7.736508092598351e-05,
      "loss": 0.2469,
      "step": 2160
    },
    {
      "epoch": 5.783012607830126,
      "grad_norm": 1.900346040725708,
      "learning_rate": 7.574492746948936e-05,
      "loss": 0.2503,
      "step": 2180
    },
    {
      "epoch": 5.836098208360982,
      "grad_norm": 2.214756965637207,
      "learning_rate": 7.421201494933742e-05,
      "loss": 0.2777,
      "step": 2200
    },
    {
      "epoch": 5.889183808891838,
      "grad_norm": 2.126214027404785,
      "learning_rate": 7.260541385374942e-05,
      "loss": 0.252,
      "step": 2220
    },
    {
      "epoch": 5.942269409422694,
      "grad_norm": 1.5106260776519775,
      "learning_rate": 7.100642183075592e-05,
      "loss": 0.2463,
      "step": 2240
    },
    {
      "epoch": 5.99535500995355,
      "grad_norm": 1.8635355234146118,
      "learning_rate": 6.941548301359979e-05,
      "loss": 0.2619,
      "step": 2260
    },
    {
      "epoch": 6.04777704047777,
      "grad_norm": 1.184257984161377,
      "learning_rate": 6.783303929867935e-05,
      "loss": 0.1769,
      "step": 2280
    },
    {
      "epoch": 6.100862641008627,
      "grad_norm": 0.733718752861023,
      "learning_rate": 6.625953022280824e-05,
      "loss": 0.1343,
      "step": 2300
    },
    {
      "epoch": 6.153948241539482,
      "grad_norm": 1.463688850402832,
      "learning_rate": 6.46953928411302e-05,
      "loss": 0.1459,
      "step": 2320
    },
    {
      "epoch": 6.207033842070339,
      "grad_norm": 1.4409551620483398,
      "learning_rate": 6.314106160572349e-05,
      "loss": 0.147,
      "step": 2340
    },
    {
      "epoch": 6.260119442601194,
      "grad_norm": 1.2658711671829224,
      "learning_rate": 6.159696824492827e-05,
      "loss": 0.1607,
      "step": 2360
    },
    {
      "epoch": 6.313205043132051,
      "grad_norm": 1.5630273818969727,
      "learning_rate": 6.006354164343046e-05,
      "loss": 0.1574,
      "step": 2380
    },
    {
      "epoch": 6.366290643662906,
      "grad_norm": 0.8001465797424316,
      "learning_rate": 5.854120772313547e-05,
      "loss": 0.1395,
      "step": 2400
    },
    {
      "epoch": 6.419376244193763,
      "grad_norm": 1.2110246419906616,
      "learning_rate": 5.703038932486484e-05,
      "loss": 0.1457,
      "step": 2420
    },
    {
      "epoch": 6.472461844724618,
      "grad_norm": 1.080958604812622,
      "learning_rate": 5.553150609090879e-05,
      "loss": 0.1455,
      "step": 2440
    },
    {
      "epoch": 6.525547445255475,
      "grad_norm": 0.7283743023872375,
      "learning_rate": 5.404497434846708e-05,
      "loss": 0.1602,
      "step": 2460
    },
    {
      "epoch": 6.57863304578633,
      "grad_norm": 1.6956254243850708,
      "learning_rate": 5.257120699401057e-05,
      "loss": 0.1515,
      "step": 2480
    },
    {
      "epoch": 6.631718646317187,
      "grad_norm": 1.2093247175216675,
      "learning_rate": 5.111061337859606e-05,
      "loss": 0.1469,
      "step": 2500
    },
    {
      "epoch": 6.684804246848042,
      "grad_norm": 0.8990018367767334,
      "learning_rate": 4.96635991941654e-05,
      "loss": 0.1427,
      "step": 2520
    },
    {
      "epoch": 6.737889847378899,
      "grad_norm": 1.0002045631408691,
      "learning_rate": 4.8230566360861296e-05,
      "loss": 0.1498,
      "step": 2540
    },
    {
      "epoch": 6.790975447909754,
      "grad_norm": 1.0795958042144775,
      "learning_rate": 4.681191291539082e-05,
      "loss": 0.1514,
      "step": 2560
    },
    {
      "epoch": 6.844061048440611,
      "grad_norm": 0.8440865278244019,
      "learning_rate": 4.540803290046712e-05,
      "loss": 0.1592,
      "step": 2580
    },
    {
      "epoch": 6.897146648971466,
      "grad_norm": 0.7501521110534668,
      "learning_rate": 4.401931625536113e-05,
      "loss": 0.157,
      "step": 2600
    },
    {
      "epoch": 6.9502322495023225,
      "grad_norm": 1.5597485303878784,
      "learning_rate": 4.26461487075925e-05,
      "loss": 0.1557,
      "step": 2620
    },
    {
      "epoch": 7.002654280026543,
      "grad_norm": 0.9821926951408386,
      "learning_rate": 4.128891166579063e-05,
      "loss": 0.1523,
      "step": 2640
    },
    {
      "epoch": 7.0557398805573985,
      "grad_norm": 0.9600592851638794,
      "learning_rate": 3.994798211375525e-05,
      "loss": 0.111,
      "step": 2660
    },
    {
      "epoch": 7.108825481088255,
      "grad_norm": 0.7657378911972046,
      "learning_rate": 3.862373250574626e-05,
      "loss": 0.1125,
      "step": 2680
    },
    {
      "epoch": 7.1619110816191105,
      "grad_norm": 0.45528626441955566,
      "learning_rate": 3.731653066303114e-05,
      "loss": 0.1113,
      "step": 2700
    },
    {
      "epoch": 7.214996682149967,
      "grad_norm": 0.8952463865280151,
      "learning_rate": 3.602673967171983e-05,
      "loss": 0.1148,
      "step": 2720
    },
    {
      "epoch": 7.2680822826808225,
      "grad_norm": 0.6658812165260315,
      "learning_rate": 3.475471778191447e-05,
      "loss": 0.1125,
      "step": 2740
    },
    {
      "epoch": 7.321167883211679,
      "grad_norm": 0.6478039026260376,
      "learning_rate": 3.3500818308202484e-05,
      "loss": 0.1102,
      "step": 2760
    },
    {
      "epoch": 7.374253483742535,
      "grad_norm": 0.47075650095939636,
      "learning_rate": 3.2265389531520494e-05,
      "loss": 0.1072,
      "step": 2780
    },
    {
      "epoch": 7.427339084273391,
      "grad_norm": 0.7022364735603333,
      "learning_rate": 3.104877460241643e-05,
      "loss": 0.1122,
      "step": 2800
    },
    {
      "epoch": 7.480424684804246,
      "grad_norm": 0.49074068665504456,
      "learning_rate": 2.9851311445736706e-05,
      "loss": 0.1115,
      "step": 2820
    },
    {
      "epoch": 7.533510285335103,
      "grad_norm": 0.9835840463638306,
      "learning_rate": 2.8673332666764708e-05,
      "loss": 0.1107,
      "step": 2840
    },
    {
      "epoch": 7.586595885865959,
      "grad_norm": 0.3046477437019348,
      "learning_rate": 2.7515165458836912e-05,
      "loss": 0.1153,
      "step": 2860
    },
    {
      "epoch": 7.639681486396815,
      "grad_norm": 0.7464580535888672,
      "learning_rate": 2.6377131512462384e-05,
      "loss": 0.1164,
      "step": 2880
    },
    {
      "epoch": 7.692767086927671,
      "grad_norm": 0.5635261535644531,
      "learning_rate": 2.5259546925970455e-05,
      "loss": 0.1118,
      "step": 2900
    },
    {
      "epoch": 7.745852687458527,
      "grad_norm": 0.7893697619438171,
      "learning_rate": 2.4162722117711744e-05,
      "loss": 0.1105,
      "step": 2920
    },
    {
      "epoch": 7.798938287989383,
      "grad_norm": 0.46080076694488525,
      "learning_rate": 2.308696173983711e-05,
      "loss": 0.114,
      "step": 2940
    },
    {
      "epoch": 7.852023888520239,
      "grad_norm": 0.9900528788566589,
      "learning_rate": 2.2032564593677774e-05,
      "loss": 0.114,
      "step": 2960
    },
    {
      "epoch": 7.905109489051095,
      "grad_norm": 0.6376620531082153,
      "learning_rate": 2.0999823546750875e-05,
      "loss": 0.1144,
      "step": 2980
    },
    {
      "epoch": 7.958195089581951,
      "grad_norm": 0.8912777900695801,
      "learning_rate": 1.998902545141319e-05,
      "loss": 0.1177,
      "step": 3000
    },
    {
      "epoch": 8.010617120106172,
      "grad_norm": 0.454673171043396,
      "learning_rate": 1.900045106518532e-05,
      "loss": 0.1075,
      "step": 3020
    },
    {
      "epoch": 8.063702720637027,
      "grad_norm": 0.4499535858631134,
      "learning_rate": 1.8034374972769198e-05,
      "loss": 0.0996,
      "step": 3040
    },
    {
      "epoch": 8.116788321167883,
      "grad_norm": 0.4753228425979614,
      "learning_rate": 1.709106550977968e-05,
      "loss": 0.0945,
      "step": 3060
    },
    {
      "epoch": 8.16987392169874,
      "grad_norm": 0.575264573097229,
      "learning_rate": 1.617078468821236e-05,
      "loss": 0.0952,
      "step": 3080
    },
    {
      "epoch": 8.222959522229596,
      "grad_norm": 0.43927159905433655,
      "learning_rate": 1.5273788123667366e-05,
      "loss": 0.0946,
      "step": 3100
    },
    {
      "epoch": 8.27604512276045,
      "grad_norm": 0.7075390815734863,
      "learning_rate": 1.4400324964350076e-05,
      "loss": 0.0945,
      "step": 3120
    },
    {
      "epoch": 8.329130723291307,
      "grad_norm": 0.6302489638328552,
      "learning_rate": 1.3550637821868284e-05,
      "loss": 0.0971,
      "step": 3140
    },
    {
      "epoch": 8.382216323822163,
      "grad_norm": 0.5074945092201233,
      "learning_rate": 1.2724962703844611e-05,
      "loss": 0.0974,
      "step": 3160
    },
    {
      "epoch": 8.43530192435302,
      "grad_norm": 0.4995182156562805,
      "learning_rate": 1.1923528948363506e-05,
      "loss": 0.1024,
      "step": 3180
    },
    {
      "epoch": 8.488387524883874,
      "grad_norm": 0.5268872976303101,
      "learning_rate": 1.1146559160270875e-05,
      "loss": 0.1022,
      "step": 3200
    },
    {
      "epoch": 8.541473125414731,
      "grad_norm": 0.653229296207428,
      "learning_rate": 1.039426914934346e-05,
      "loss": 0.0989,
      "step": 3220
    },
    {
      "epoch": 8.594558725945587,
      "grad_norm": 0.3821951746940613,
      "learning_rate": 9.666867870346052e-06,
      "loss": 0.0989,
      "step": 3240
    },
    {
      "epoch": 8.647644326476444,
      "grad_norm": 0.5105462074279785,
      "learning_rate": 8.964557364992609e-06,
      "loss": 0.1002,
      "step": 3260
    },
    {
      "epoch": 8.700729927007298,
      "grad_norm": 0.4844050407409668,
      "learning_rate": 8.287532705827394e-06,
      "loss": 0.0952,
      "step": 3280
    },
    {
      "epoch": 8.753815527538155,
      "grad_norm": 0.45276474952697754,
      "learning_rate": 7.635981942042037e-06,
      "loss": 0.0982,
      "step": 3300
    },
    {
      "epoch": 8.806901128069011,
      "grad_norm": 0.5730509161949158,
      "learning_rate": 7.0100860472433025e-06,
      "loss": 0.097,
      "step": 3320
    },
    {
      "epoch": 8.859986728599868,
      "grad_norm": 0.4670916199684143,
      "learning_rate": 6.410018869186196e-06,
      "loss": 0.0996,
      "step": 3340
    },
    {
      "epoch": 8.913072329130724,
      "grad_norm": 0.6440898776054382,
      "learning_rate": 5.835947081486237e-06,
      "loss": 0.094,
      "step": 3360
    },
    {
      "epoch": 8.966157929661579,
      "grad_norm": 0.40826526284217834,
      "learning_rate": 5.288030137324562e-06,
      "loss": 0.1013,
      "step": 3380
    },
    {
      "epoch": 9.0185799601858,
      "grad_norm": 0.34091293811798096,
      "learning_rate": 4.766420225158497e-06,
      "loss": 0.0927,
      "step": 3400
    },
    {
      "epoch": 9.071665560716655,
      "grad_norm": 0.45223695039749146,
      "learning_rate": 4.271262226449879e-06,
      "loss": 0.0943,
      "step": 3420
    },
    {
      "epoch": 9.124751161247511,
      "grad_norm": 0.4991879165172577,
      "learning_rate": 3.802693675423075e-06,
      "loss": 0.0943,
      "step": 3440
    },
    {
      "epoch": 9.177836761778368,
      "grad_norm": 0.5107866525650024,
      "learning_rate": 3.360844720863765e-06,
      "loss": 0.0972,
      "step": 3460
    },
    {
      "epoch": 9.230922362309224,
      "grad_norm": 0.3791801333427429,
      "learning_rate": 2.945838089968922e-06,
      "loss": 0.0932,
      "step": 3480
    },
    {
      "epoch": 9.28400796284008,
      "grad_norm": 0.29591304063796997,
      "learning_rate": 2.557789054258464e-06,
      "loss": 0.0921,
      "step": 3500
    },
    {
      "epoch": 9.337093563370935,
      "grad_norm": 0.4834756553173065,
      "learning_rate": 2.196805397557533e-06,
      "loss": 0.0889,
      "step": 3520
    },
    {
      "epoch": 9.390179163901792,
      "grad_norm": 0.44961443543434143,
      "learning_rate": 1.8629873860586566e-06,
      "loss": 0.0913,
      "step": 3540
    },
    {
      "epoch": 9.443264764432648,
      "grad_norm": 0.4274442493915558,
      "learning_rate": 1.556427740472033e-06,
      "loss": 0.0917,
      "step": 3560
    },
    {
      "epoch": 9.496350364963504,
      "grad_norm": 0.5764850974082947,
      "learning_rate": 1.2772116102715448e-06,
      "loss": 0.0909,
      "step": 3580
    },
    {
      "epoch": 9.549435965494359,
      "grad_norm": 0.42881351709365845,
      "learning_rate": 1.025416550043834e-06,
      "loss": 0.0883,
      "step": 3600
    },
    {
      "epoch": 9.602521566025215,
      "grad_norm": 0.4845898449420929,
      "learning_rate": 8.011124979467988e-07,
      "loss": 0.0927,
      "step": 3620
    },
    {
      "epoch": 9.655607166556072,
      "grad_norm": 0.3601318895816803,
      "learning_rate": 6.043617562837889e-07,
      "loss": 0.095,
      "step": 3640
    },
    {
      "epoch": 9.708692767086928,
      "grad_norm": 0.5150774717330933,
      "learning_rate": 4.3521897419855907e-07,
      "loss": 0.091,
      "step": 3660
    },
    {
      "epoch": 9.761778367617783,
      "grad_norm": 0.43655914068222046,
      "learning_rate": 2.9373113249602234e-07,
      "loss": 0.0967,
      "step": 3680
    },
    {
      "epoch": 9.81486396814864,
      "grad_norm": 0.37005752325057983,
      "learning_rate": 1.7993753059297735e-07,
      "loss": 0.093,
      "step": 3700
    },
    {
      "epoch": 9.867949568679496,
      "grad_norm": 0.35322269797325134,
      "learning_rate": 9.386977560232879e-08,
      "loss": 0.0876,
      "step": 3720
    },
    {
      "epoch": 9.921035169210352,
      "grad_norm": 0.48814326524734497,
      "learning_rate": 3.555177355399897e-08,
      "loss": 0.0939,
      "step": 3740
    },
    {
      "epoch": 9.974120769741207,
      "grad_norm": 0.4541269838809967,
      "learning_rate": 4.999722754783775e-09,
      "loss": 0.0915,
      "step": 3760
    }
  ],
  "logging_steps": 20,
  "max_steps": 3770,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8.352756169721856e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
